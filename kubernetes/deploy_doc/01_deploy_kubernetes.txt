# --author-- = "lilingchengCN"

此部分介绍kuberneters的搭建过程，命令行及配置：

一、搭建kuberneters cluster
	1. 硬件配置：
	master	10.0.0.25	kubernetes主节点
	node1	10.0.0.102	kubernetes从节点
	node2	10.0.0.98	kubernetes从节点
	
	2. 安装docker：
	curl -s https://get.docker.com/ | sudo sh

	3. 关闭swap、防火墙等
	systemctl stop firewalld.service
	swapoff -a

	4. 安装kubelet、kubeadm 和 kubectl
	（本处使用kubeadm搭建kuberneters）
	使用aliyun镜像代替官方镜像
	cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
		deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
		EOF

	apt-get update
	apt-get install -y kubelet kubeadm kubectl --allow-unauthenticated

	5. 初始化master
	kubeadm config images list
		k8s.gcr.io/kube-apiserver:v1.15.0
		k8s.gcr.io/kube-controller-manager:v1.15.0
		k8s.gcr.io/kube-scheduler:v1.15.0
		k8s.gcr.io/kube-proxy:v1.15.0
		k8s.gcr.io/pause:3.1
		k8s.gcr.io/etcd:3.3.10
		k8s.gcr.io/coredns:1.3.1

	可以直接通过docker pull 镜像仓库地址/需要的镜像下载，然后通过docker tag 原标签 
	需要的标签将镜像的标签改为kubeadm init需要的标签就可以了，针对上面列出的我需要的镜像，
	下面这些命令完成了镜像的下载和标签更改：
	docker pull mirrorgooglecontainers/kube-apiserver:v1.15.0
	docker tag mirrorgooglecontainers/kube-apiserver:v1.15.0 k8s.gcr.io/kube-apiserver:v1.15.0
	docker pull mirrorgooglecontainers/kube-controller-manager:v1.15.0
	docker tag mirrorgooglecontainers/kube-controller-manager:v1.15.0 k8s.gcr.io/kube-controller-manager:v1.15.0
	docker pull mirrorgooglecontainers/kube-scheduler:v1.15.0
	docker tag mirrorgooglecontainers/kube-scheduler:v1.15.0 k8s.gcr.io/kube-scheduler:v1.15.0
	docker pull mirrorgooglecontainers/kube-proxy:v1.15.0
	docker tag mirrorgooglecontainers/kube-proxy:v1.15.0 k8s.gcr.io/kube-proxy:v1.15.0
	docker pull mirrorgooglecontainers/pause:3.1
	docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1
	docker pull mirrorgooglecontainers/etcd:3.3.10
	docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10
	docker pull registry.cn-hangzhou.aliyuncs.com/coredns:1.3.1
	docker tag registry.cn-hangzhou.aliyuncs.com/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1
	
	上面的命令完成后，kubeadm init命令就可以顺利完成了。
	kubeadm init

	使得各个pod之间可以通信，有多种选择，每中网络插件支持的架构有所不同，
	我选了Weave Net，Weave Net 只能在amd64, arm 和 arm64 上运行。可以通过
	下面两条命令完成安装：
	export kubever=$(kubectl version | base64 | tr -d '\n')
	kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"
	
	6. 从节点加入cluster
	在执行kubeadm init的返回结果末尾会给出kubeadm join --token <token> <master-ip>:<master-port> 
	--discovery-token-ca-cert-hash sha256:<hash>格式的命令，直接复制在自节点root模式下执行就可以完成加入。
	都加入完成过一分钟返回master通过kubectl get nodes查看所有节点的状态。显示应该如下：
		(我的命令行为：sudo kubeadm join 10.0.0.25:6443 --token vuaevw.uqqud15cvebleben \
		--discovery-token-ca-cert-hash sha256:a9dc2049345b5ba3fb8524ec191ce94ea6b34c705ddb818ac1b17e3cd2a417d6
		插入到各个node节点)
	其结果为：
		ubuntu@ubuntu4:~/data_eureka_docker$ kubectl get node
		NAME      STATUS   ROLES    AGE   VERSION
		ubuntu2   Ready    <none>   42h   v1.15.0
		ubuntu3   Ready    <none>   43h   v1.15.0
		ubuntu4   Ready    master   44h   v1.15.0

	7. tips
	从节点加入成功，但是通过kubectl get nodes查看，master的STATUS处于Ready，但是从节点全部是NotReady。

	这个坑我踩了很久才踩平，尝试过很多方法后发现，不光是kubelet、kubeadm 和 kubectl，连kubeadm init需要
	的那些镜像也需要在所有节点上安装。这样节点在介入后才能正常运行。

	执行kubeadm reset重置后重新初始化时出错

	重置后会有一些配置残留，主要是etcd集群留下的，删除/var/lib/etcd文件夹就可以重新初始化了。

	coreDNS pod一直处于pending状态

	可能是由于安装网络插件之前，kubeadm init需要的参数没有添加。

	使用kubectl delete pods xxx删除对应的pod,提示删除成功，但是立马又回生成一个

	要先删除对应deployment
